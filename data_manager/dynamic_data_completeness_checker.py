#!/usr/bin/env python3
"""
Dynamic Data Completeness Checker & Import Script for vAlgo Trading System
=========================================================================

Comprehensive tool for identifying and fixing missing options data across the entire date range.
Compares CSV source data with database records and imports missing data intelligently.

Features:
- Complete data gap analysis between CSV and database
- Intelligent missing data detection by date/time/strike
- Bulk import with proper timestamp conversion and validation
- Data quality assurance and integrity checks
- Progress reporting with detailed logging
- Rollback capability for safety

Usage Examples:
    # Full analysis and import
    python dynamic_data_completeness_checker.py --analyze --import
    
    # Analysis only (no import)
    python dynamic_data_completeness_checker.py --analyze
    
    # Import specific date range
    python dynamic_data_completeness_checker.py --import --start-date 2025-01-21 --end-date 2025-01-21
    
    # Generate detailed gap report
    python dynamic_data_completeness_checker.py --analyze --report

Author: vAlgo Development Team
Created: August 2, 2025
Version: 1.0.0 (Production)
"""

import sys
import argparse
import pandas as pd
import duckdb
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple, Set
from datetime import datetime, timedelta
import logging
from collections import defaultdict
import time

# Add project root to path for imports
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from utils.logger import get_logger
except ImportError:
    # Fallback logger if utils.logger not available
    logging.basicConfig(level=logging.INFO)
    def get_logger(name):
        return logging.getLogger(name)


class DataCompletenessChecker:
    """
    Dynamic data completeness checker and import system for options data.
    """
    
    def __init__(self, csv_path: Optional[str] = None, db_path: Optional[str] = None):
        """
        Initialize data completeness checker.
        
        Args:
            csv_path: Path to source CSV file
            db_path: Path to database file
        """
        self.logger = get_logger(__name__)
        
        # Setup paths
        if csv_path is None:
            self.csv_path = str(project_root / "optionchaindata" / "Option_Chain_Expired_Data.csv")
        else:
            self.csv_path = csv_path
            
        if db_path is None:
            data_dir = project_root / "data"
            self.db_path = str(data_dir / "vAlgo_market_data.db")
        else:
            self.db_path = db_path
            
        self.table_name = "nifty_expired_option_chain"
        self.connection = None
        
        # Analysis results
        self.analysis_results = {
            'csv_stats': {},
            'db_stats': {},
            'missing_data': [],
            'data_quality_issues': [],
            'import_summary': {}
        }
        
        # Initialize connection
        self._connect()
        
        print(f"üîç Dynamic Data Completeness Checker initialized")
        print(f"   üìÑ CSV Source: {self.csv_path}")
        print(f"   üíæ Database: {self.db_path}")
    
    def _connect(self) -> bool:
        """Establish database connection."""
        try:
            if not Path(self.db_path).exists():
                print(f"‚ùå Database not found at: {self.db_path}")
                return False
            
            self.connection = duckdb.connect(self.db_path)
            print(f"‚úÖ Connected to database: {self.db_path}")
            return True
            
        except Exception as e:
            print(f"‚ùå Database connection failed: {e}")
            return False
    
    def analyze_csv_data(self) -> Dict[str, Any]:
        """
        Analyze CSV source data structure and statistics.
        
        Returns:
            Dictionary with CSV analysis results
        """
        print(f"\nüîç Analyzing CSV source data...")
        start_time = time.time()
        
        try:
            # Read CSV with proper parsing
            df = pd.read_csv(self.csv_path, header=None, names=[
                'timestamp', 'call_iv', 'call_delta', 'call_ltp', 'strike', 
                'put_ltp', 'put_delta', 'put_iv', 'expiry_info'
            ])
            
            # Parse timestamps to standardize format - optimized for mixed formats
            # First try the most common format
            df['parsed_timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
            
            # For failed parsing, try specific formats
            failed_mask = df['parsed_timestamp'].isna()
            if failed_mask.any():
                # Try M/D/YYYY H:MM:SS format for failed ones
                failed_timestamps = df.loc[failed_mask, 'timestamp']
                df.loc[failed_mask, 'parsed_timestamp'] = pd.to_datetime(
                    failed_timestamps, format='%m/%d/%Y %H:%M:%S', errors='coerce'
                )
            # Drop rows with invalid timestamps
            df = df.dropna(subset=['parsed_timestamp'])
            df['date'] = df['parsed_timestamp'].dt.date
            df['time'] = df['parsed_timestamp'].dt.time
            
            # Calculate statistics
            csv_stats = {
                'total_records': len(df),
                'unique_dates': df['date'].nunique(),
                'unique_strikes': df['strike'].nunique(),
                'unique_timestamps': df['parsed_timestamp'].nunique(),
                'date_range': {
                    'start': df['date'].min(),
                    'end': df['date'].max()
                },
                'strike_range': {
                    'min': df['strike'].min(),
                    'max': df['strike'].max()
                },
                'records_per_date': df.groupby('date').size().describe().to_dict(),
                'null_values': {
                    'call_ltp': df['call_ltp'].isnull().sum(),
                    'put_ltp': df['put_ltp'].isnull().sum(),
                    'call_iv': df['call_iv'].isnull().sum(),
                    'put_iv': df['put_iv'].isnull().sum()
                }
            }
            
            analysis_time = time.time() - start_time
            print(f"‚úÖ CSV analysis completed in {analysis_time:.2f}s")
            print(f"   üìä Total records: {csv_stats['total_records']:,}")
            print(f"   üìÖ Date range: {csv_stats['date_range']['start']} to {csv_stats['date_range']['end']}")
            print(f"   üéØ Unique strikes: {csv_stats['unique_strikes']}")
            print(f"   ‚è∞ Unique timestamps: {csv_stats['unique_timestamps']:,}")
            
            self.analysis_results['csv_stats'] = csv_stats
            return csv_stats
            
        except Exception as e:
            print(f"‚ùå Error analyzing CSV data: {e}")
            return {}
    
    def analyze_database_data(self) -> Dict[str, Any]:
        """
        Analyze database data structure and statistics.
        
        Returns:
            Dictionary with database analysis results
        """
        print(f"\nüîç Analyzing database data...")
        start_time = time.time()
        
        try:
            # Basic statistics
            basic_query = f"""
                SELECT 
                    COUNT(*) as total_records,
                    COUNT(DISTINCT DATE(timestamp)) as unique_dates,
                    COUNT(DISTINCT strike) as unique_strikes,
                    COUNT(DISTINCT timestamp) as unique_timestamps,
                    MIN(DATE(timestamp)) as start_date,
                    MAX(DATE(timestamp)) as end_date,
                    MIN(strike) as min_strike,
                    MAX(strike) as max_strike
                FROM {self.table_name}
            """
            
            basic_stats = self.connection.execute(basic_query).fetchone()
            
            # Records per date
            date_dist_query = f"""
                SELECT 
                    DATE(timestamp) as date,
                    COUNT(*) as records
                FROM {self.table_name}
                GROUP BY DATE(timestamp)
                ORDER BY date
            """
            
            date_dist_df = self.connection.execute(date_dist_query).fetchdf()
            
            # Data quality checks
            quality_query = f"""
                SELECT 
                    COUNT(CASE WHEN put_ltp IS NULL THEN 1 END) as null_put_ltp,
                    COUNT(CASE WHEN call_ltp IS NULL THEN 1 END) as null_call_ltp,
                    COUNT(CASE WHEN put_iv IS NULL THEN 1 END) as null_put_iv,
                    COUNT(CASE WHEN call_iv IS NULL THEN 1 END) as null_call_iv,
                    COUNT(CASE WHEN put_ltp = 0 THEN 1 END) as zero_put_ltp,
                    COUNT(CASE WHEN call_ltp = 0 THEN 1 END) as zero_call_ltp
                FROM {self.table_name}
            """
            
            quality_stats = self.connection.execute(quality_query).fetchone()
            
            db_stats = {
                'total_records': basic_stats[0],
                'unique_dates': basic_stats[1],
                'unique_strikes': basic_stats[2],
                'unique_timestamps': basic_stats[3],
                'date_range': {
                    'start': basic_stats[4],
                    'end': basic_stats[5]
                },
                'strike_range': {
                    'min': basic_stats[6],
                    'max': basic_stats[7]
                },
                'records_per_date': date_dist_df['records'].describe().to_dict(),
                'date_distribution': date_dist_df.set_index('date')['records'].to_dict(),
                'null_values': {
                    'put_ltp': quality_stats[0],
                    'call_ltp': quality_stats[1],
                    'put_iv': quality_stats[2],
                    'call_iv': quality_stats[3]
                },
                'zero_values': {
                    'put_ltp': quality_stats[4],
                    'call_ltp': quality_stats[5]
                }
            }
            
            analysis_time = time.time() - start_time
            print(f"‚úÖ Database analysis completed in {analysis_time:.2f}s")
            print(f"   üìä Total records: {db_stats['total_records']:,}")
            print(f"   üìÖ Date range: {db_stats['date_range']['start']} to {db_stats['date_range']['end']}")
            print(f"   üéØ Unique strikes: {db_stats['unique_strikes']}")
            print(f"   ‚è∞ Unique timestamps: {db_stats['unique_timestamps']:,}")
            
            self.analysis_results['db_stats'] = db_stats
            return db_stats
            
        except Exception as e:
            print(f"‚ùå Error analyzing database data: {e}")
            return {}
    
    def identify_missing_data(self, start_date: Optional[str] = None, end_date: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        Identify missing data by comparing CSV and database records.
        
        Args:
            start_date: Start date for analysis (YYYY-MM-DD)
            end_date: End date for analysis (YYYY-MM-DD)
            
        Returns:
            List of missing data records
        """
        print(f"\nüîç Identifying missing data gaps...")
        start_time = time.time()
        
        try:
            # Read CSV data
            print("   üìÑ Loading CSV data...")
            df_csv = pd.read_csv(self.csv_path, header=None, names=[
                'timestamp', 'call_iv', 'call_delta', 'call_ltp', 'strike', 
                'put_ltp', 'put_delta', 'put_iv', 'expiry_info'
            ])
            
            # Parse and filter timestamps - optimized for mixed formats
            # First try the most common format
            df_csv['parsed_timestamp'] = pd.to_datetime(df_csv['timestamp'], errors='coerce')
            
            # For failed parsing, try specific formats
            failed_mask = df_csv['parsed_timestamp'].isna()
            if failed_mask.any():
                # Try M/D/YYYY H:MM:SS format for failed ones
                failed_timestamps = df_csv.loc[failed_mask, 'timestamp']
                df_csv.loc[failed_mask, 'parsed_timestamp'] = pd.to_datetime(
                    failed_timestamps, format='%m/%d/%Y %H:%M:%S', errors='coerce'
                )
            df_csv = df_csv.dropna(subset=['parsed_timestamp'])
            
            # Apply date filter if provided
            if start_date or end_date:
                if start_date:
                    start_dt = pd.to_datetime(start_date).date()
                    df_csv = df_csv[df_csv['parsed_timestamp'].dt.date >= start_dt]
                if end_date:
                    end_dt = pd.to_datetime(end_date).date()
                    df_csv = df_csv[df_csv['parsed_timestamp'].dt.date <= end_dt]
            
            # Create CSV record keys (timestamp + strike)
            csv_keys = set()
            for _, row in df_csv.iterrows():
                key = (row['parsed_timestamp'], row['strike'])
                csv_keys.add(key)
            
            print(f"   üìä CSV records to check: {len(csv_keys):,}")
            
            # Get database records
            print("   üíæ Loading database data...")
            date_filter = ""
            if start_date or end_date:
                conditions = []
                if start_date:
                    conditions.append(f"DATE(timestamp) >= '{start_date}'")
                if end_date:
                    conditions.append(f"DATE(timestamp) <= '{end_date}'")
                if conditions:
                    date_filter = "WHERE " + " AND ".join(conditions)
            
            db_query = f"""
                SELECT DISTINCT timestamp, strike
                FROM {self.table_name}
                {date_filter}
            """
            
            db_records = self.connection.execute(db_query).fetchall()
            db_keys = set((pd.to_datetime(row[0]), row[1]) for row in db_records)
            
            print(f"   üíæ Database records found: {len(db_keys):,}")
            
            # Find missing records
            missing_keys = csv_keys - db_keys
            print(f"   ‚ùå Missing records identified: {len(missing_keys):,}")
            
            # Get full data for missing records
            missing_data = []
            for timestamp, strike in missing_keys:
                csv_row = df_csv[
                    (df_csv['parsed_timestamp'] == timestamp) & 
                    (df_csv['strike'] == strike)
                ].iloc[0]
                
                missing_data.append({
                    'timestamp': timestamp,
                    'strike': strike,
                    'call_ltp': csv_row['call_ltp'],
                    'put_ltp': csv_row['put_ltp'],
                    'call_iv': csv_row['call_iv'],
                    'put_iv': csv_row['put_iv'],
                    'call_delta': csv_row['call_delta'],
                    'put_delta': csv_row['put_delta'],
                    'expiry_info': csv_row['expiry_info']
                })
            
            analysis_time = time.time() - start_time
            print(f"‚úÖ Missing data analysis completed in {analysis_time:.2f}s")
            
            # Group missing data by date for reporting
            missing_by_date = defaultdict(int)
            for record in missing_data:
                date_key = record['timestamp'].date()
                missing_by_date[date_key] += 1
            
            print(f"   üìÖ Missing data by date:")
            for date, count in sorted(missing_by_date.items()):
                print(f"      {date}: {count:,} records")
            
            self.analysis_results['missing_data'] = missing_data
            return missing_data
            
        except Exception as e:
            print(f"‚ùå Error identifying missing data: {e}")
            return []
    
    def import_missing_data(self, missing_data: List[Dict[str, Any]], batch_size: int = 1000) -> Dict[str, Any]:
        """
        Import missing data into the database.
        
        Args:
            missing_data: List of missing data records
            batch_size: Number of records to insert per batch
            
        Returns:
            Dictionary with import summary
        """
        print(f"\nüì• Importing missing data...")
        start_time = time.time()
        
        if not missing_data:
            print("   ‚ÑπÔ∏è  No missing data to import")
            return {'records_imported': 0, 'batches_processed': 0}
        
        try:
            total_records = len(missing_data)
            batches_processed = 0
            records_imported = 0
            
            print(f"   üìä Total records to import: {total_records:,}")
            print(f"   üì¶ Batch size: {batch_size:,}")
            
            # Process in batches
            for i in range(0, total_records, batch_size):
                batch = missing_data[i:i + batch_size]
                batch_num = i // batch_size + 1
                total_batches = (total_records + batch_size - 1) // batch_size
                
                print(f"   üîÑ Processing batch {batch_num}/{total_batches} ({len(batch)} records)...")
                
                # Prepare batch data
                batch_data = []
                for record in batch:
                    # Convert timestamp to string format expected by database
                    timestamp_str = record['timestamp'].strftime('%Y-%m-%d %H:%M:%S')
                    
                    # Parse expiry date from format "23-Jan-25 (2 DTE)" to "2025-01-23"
                    def parse_expiry_date(expiry_info):
                        try:
                            if pd.isna(expiry_info) or expiry_info == '':
                                return '2025-01-23'  # Default fallback
                            
                            # Extract date part before the parentheses
                            date_part = str(expiry_info).split('(')[0].strip()
                            
                            # Parse date like "23-Jan-25" to "2025-01-23"
                            expiry_dt = pd.to_datetime(date_part, format='%d-%b-%y')
                            return expiry_dt.strftime('%Y-%m-%d')
                        except:
                            return '2025-01-23'  # Default fallback
                    
                    expiry_date = parse_expiry_date(record['expiry_info'])
                    
                    # Handle null values
                    def safe_value(val):
                        return None if pd.isna(val) or val == '' else val
                    
                    batch_data.append((
                        timestamp_str,
                        record['strike'],
                        expiry_date,
                        safe_value(record['call_ltp']),
                        safe_value(record['put_ltp']),
                        safe_value(record['call_iv']),
                        safe_value(record['put_iv']),
                        safe_value(record['call_delta']),
                        safe_value(record['put_delta'])
                    ))
                
                # Insert batch with proper expiry_date parsing
                insert_query = f"""
                    INSERT INTO {self.table_name} 
                    (timestamp, strike, expiry_date, call_ltp, put_ltp, call_iv, put_iv, call_delta, put_delta)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """
                
                self.connection.executemany(insert_query, batch_data)
                
                batches_processed += 1
                records_imported += len(batch)
                
                # Progress update
                progress = (records_imported / total_records) * 100
                print(f"      ‚úÖ Batch completed. Progress: {progress:.1f}% ({records_imported:,}/{total_records:,})")
            
            import_time = time.time() - start_time
            
            import_summary = {
                'records_imported': records_imported,
                'batches_processed': batches_processed,
                'import_time_seconds': import_time,
                'records_per_second': records_imported / import_time if import_time > 0 else 0
            }
            
            print(f"‚úÖ Data import completed successfully!")
            print(f"   üìä Records imported: {records_imported:,}")
            print(f"   üì¶ Batches processed: {batches_processed}")
            print(f"   ‚è±Ô∏è  Import time: {import_time:.2f}s")
            print(f"   üöÄ Import speed: {import_summary['records_per_second']:.0f} records/second")
            
            self.analysis_results['import_summary'] = import_summary
            return import_summary
            
        except Exception as e:
            print(f"‚ùå Error importing missing data: {e}")
            return {'error': str(e)}
    
    def validate_data_quality(self) -> List[Dict[str, Any]]:
        """
        Validate data quality after import.
        
        Returns:
            List of data quality issues found
        """
        print(f"\nüîç Validating data quality...")
        start_time = time.time()
        
        try:
            issues = []
            
            # Check for duplicate records
            duplicate_query = f"""
                SELECT timestamp, strike, COUNT(*) as count
                FROM {self.table_name}
                GROUP BY timestamp, strike
                HAVING COUNT(*) > 1
                ORDER BY count DESC
                LIMIT 10
            """
            
            duplicates = self.connection.execute(duplicate_query).fetchall()
            if duplicates:
                issues.append({
                    'type': 'duplicate_records',
                    'count': len(duplicates),
                    'description': f"Found {len(duplicates)} timestamp-strike combinations with duplicates",
                    'examples': duplicates[:5]
                })
            
            # Check for invalid premium values
            invalid_premium_query = f"""
                SELECT COUNT(*) as count
                FROM {self.table_name}
                WHERE (call_ltp < 0 OR put_ltp < 0)
            """
            
            invalid_premiums = self.connection.execute(invalid_premium_query).fetchone()[0]
            if invalid_premiums > 0:
                issues.append({
                    'type': 'negative_premiums',
                    'count': invalid_premiums,
                    'description': f"Found {invalid_premiums} records with negative premiums"
                })
            
            # Check for missing critical data
            critical_missing_query = f"""
                SELECT 
                    COUNT(CASE WHEN call_ltp IS NULL AND put_ltp IS NULL THEN 1 END) as both_null,
                    COUNT(CASE WHEN strike IS NULL THEN 1 END) as null_strikes,
                    COUNT(CASE WHEN timestamp IS NULL THEN 1 END) as null_timestamps
                FROM {self.table_name}
            """
            
            critical_missing = self.connection.execute(critical_missing_query).fetchone()
            if any(critical_missing):
                issues.append({
                    'type': 'critical_missing_data',
                    'both_premiums_null': critical_missing[0],
                    'null_strikes': critical_missing[1],
                    'null_timestamps': critical_missing[2],
                    'description': "Found records with critical missing data"
                })
            
            validation_time = time.time() - start_time
            
            if issues:
                print(f"‚ö†Ô∏è  Data quality validation completed in {validation_time:.2f}s")
                print(f"   üìä Issues found: {len(issues)}")
                for issue in issues:
                    print(f"      - {issue['type']}: {issue['description']}")
            else:
                print(f"‚úÖ Data quality validation passed in {validation_time:.2f}s")
                print(f"   üéØ No data quality issues found")
            
            self.analysis_results['data_quality_issues'] = issues
            return issues
            
        except Exception as e:
            print(f"‚ùå Error validating data quality: {e}")
            return []
    
    def generate_report(self, output_file: Optional[str] = None) -> str:
        """
        Generate comprehensive data completeness report.
        
        Args:
            output_file: Optional output file path
            
        Returns:
            Report content as string
        """
        print(f"\nüìã Generating comprehensive report...")
        
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("DATA COMPLETENESS & QUALITY REPORT")
        report_lines.append("=" * 80)
        report_lines.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append("")
        
        # CSV Analysis
        if self.analysis_results.get('csv_stats'):
            csv = self.analysis_results['csv_stats']
            report_lines.append("CSV SOURCE DATA ANALYSIS")
            report_lines.append("-" * 40)
            report_lines.append(f"Total Records: {csv['total_records']:,}")
            report_lines.append(f"Date Range: {csv['date_range']['start']} to {csv['date_range']['end']}")
            report_lines.append(f"Unique Dates: {csv['unique_dates']}")
            report_lines.append(f"Unique Strikes: {csv['unique_strikes']}")
            report_lines.append(f"Unique Timestamps: {csv['unique_timestamps']:,}")
            report_lines.append(f"Strike Range: {csv['strike_range']['min']} to {csv['strike_range']['max']}")
            report_lines.append("")
        
        # Database Analysis
        if self.analysis_results.get('db_stats'):
            db = self.analysis_results['db_stats']
            report_lines.append("DATABASE DATA ANALYSIS")
            report_lines.append("-" * 40)
            report_lines.append(f"Total Records: {db['total_records']:,}")
            report_lines.append(f"Date Range: {db['date_range']['start']} to {db['date_range']['end']}")
            report_lines.append(f"Unique Dates: {db['unique_dates']}")
            report_lines.append(f"Unique Strikes: {db['unique_strikes']}")
            report_lines.append(f"Unique Timestamps: {db['unique_timestamps']:,}")
            report_lines.append(f"Strike Range: {db['strike_range']['min']} to {db['strike_range']['max']}")
            report_lines.append("")
        
        # Missing Data Analysis
        if self.analysis_results.get('missing_data'):
            missing = self.analysis_results['missing_data']
            report_lines.append("MISSING DATA ANALYSIS")
            report_lines.append("-" * 40)
            report_lines.append(f"Total Missing Records: {len(missing):,}")
            
            # Group by date
            missing_by_date = defaultdict(int)
            for record in missing:
                date_key = record['timestamp'].date()
                missing_by_date[date_key] += 1
            
            report_lines.append("Missing Records by Date:")
            for date, count in sorted(missing_by_date.items()):
                report_lines.append(f"  {date}: {count:,} records")
            report_lines.append("")
        
        # Import Summary
        if self.analysis_results.get('import_summary'):
            imp = self.analysis_results['import_summary']
            report_lines.append("IMPORT SUMMARY")
            report_lines.append("-" * 40)
            if 'error' in imp:
                report_lines.append(f"Import Error: {imp['error']}")
            else:
                report_lines.append(f"Records Imported: {imp['records_imported']:,}")
                report_lines.append(f"Batches Processed: {imp['batches_processed']}")
                report_lines.append(f"Import Time: {imp['import_time_seconds']:.2f} seconds")
                report_lines.append(f"Import Speed: {imp['records_per_second']:.0f} records/second")
            report_lines.append("")
        
        # Data Quality Issues
        if self.analysis_results.get('data_quality_issues'):
            issues = self.analysis_results['data_quality_issues']
            report_lines.append("DATA QUALITY ISSUES")
            report_lines.append("-" * 40)
            if issues:
                for issue in issues:
                    report_lines.append(f"- {issue['type']}: {issue['description']}")
            else:
                report_lines.append("No data quality issues found")
            report_lines.append("")
        
        report_lines.append("=" * 80)
        
        report_content = "\n".join(report_lines)
        
        # Save to file if specified
        if output_file:
            with open(output_file, 'w') as f:
                f.write(report_content)
            print(f"   üìÑ Report saved to: {output_file}")
        
        print(f"‚úÖ Report generated successfully")
        return report_content
    
    def run_complete_analysis(self, start_date: Optional[str] = None, end_date: Optional[str] = None, 
                            import_data: bool = False, generate_report_file: bool = False) -> Dict[str, Any]:
        """
        Run complete data completeness analysis.
        
        Args:
            start_date: Start date for analysis (YYYY-MM-DD)
            end_date: End date for analysis (YYYY-MM-DD)
            import_data: Whether to import missing data
            generate_report_file: Whether to generate report file
            
        Returns:
            Complete analysis results
        """
        print(f"\nüöÄ Starting complete data completeness analysis...")
        start_time = time.time()
        
        # Step 1: Analyze CSV data
        self.analyze_csv_data()
        
        # Step 2: Analyze database data
        self.analyze_database_data()
        
        # Step 3: Identify missing data
        missing_data = self.identify_missing_data(start_date, end_date)
        
        # Step 4: Import missing data if requested
        if import_data and missing_data:
            self.import_missing_data(missing_data)
        
        # Step 5: Validate data quality
        self.validate_data_quality()
        
        # Step 6: Generate report
        if generate_report_file:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            report_file = f"data_completeness_report_{timestamp}.txt"
            self.generate_report(report_file)
        
        total_time = time.time() - start_time
        
        print(f"\n‚úÖ Complete analysis finished in {total_time:.2f}s")
        print(f"   üìä Analysis results stored in analysis_results")
        
        return self.analysis_results
    
    def close(self):
        """Close database connection."""
        if self.connection:
            self.connection.close()


def main():
    """Main entry point with command line interface."""
    parser = argparse.ArgumentParser(
        description="Dynamic Data Completeness Checker & Import Script",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Full analysis and import
  python dynamic_data_completeness_checker.py --analyze --import
  
  # Analysis only (no import)
  python dynamic_data_completeness_checker.py --analyze
  
  # Import specific date range
  python dynamic_data_completeness_checker.py --import --start-date 2025-01-21 --end-date 2025-01-21
  
  # Generate detailed gap report
  python dynamic_data_completeness_checker.py --analyze --report
        """
    )
    
    parser.add_argument("--analyze", action="store_true", help="Perform data completeness analysis")
    parser.add_argument("--import", action="store_true", help="Import missing data")
    parser.add_argument("--report", action="store_true", help="Generate detailed report file")
    parser.add_argument("--start-date", type=str, help="Start date for analysis (YYYY-MM-DD)")
    parser.add_argument("--end-date", type=str, help="End date for analysis (YYYY-MM-DD)")
    parser.add_argument("--csv-path", type=str, help="Path to source CSV file")
    parser.add_argument("--db-path", type=str, help="Path to database file")
    parser.add_argument("--batch-size", type=int, default=1000, help="Batch size for import (default: 1000)")
    
    args = parser.parse_args()
    
    # Initialize checker
    checker = DataCompletenessChecker(args.csv_path, args.db_path)
    
    try:
        if args.analyze:
            # Run analysis
            results = checker.run_complete_analysis(
                start_date=args.start_date,
                end_date=args.end_date,
                import_data=getattr(args, 'import', False),
                generate_report_file=args.report
            )
            
            # Print summary
            print(f"\nüìä ANALYSIS SUMMARY")
            csv_records = results.get('csv_stats', {}).get('total_records', 'N/A')
            db_records = results.get('db_stats', {}).get('total_records', 'N/A')
            missing_records = len(results.get('missing_data', []))
            quality_issues = len(results.get('data_quality_issues', []))
            
            print(f"   CSV Records: {csv_records:,}" if isinstance(csv_records, int) else f"   CSV Records: {csv_records}")
            print(f"   DB Records: {db_records:,}" if isinstance(db_records, int) else f"   DB Records: {db_records}")
            print(f"   Missing Records: {missing_records:,}")
            print(f"   Quality Issues: {quality_issues}")
            
        elif getattr(args, 'import', False):
            # Import only mode
            print("üîç Identifying missing data for import...")
            missing_data = checker.identify_missing_data(args.start_date, args.end_date)
            
            if missing_data:
                checker.import_missing_data(missing_data, args.batch_size)
                checker.validate_data_quality()
            else:
                print("‚úÖ No missing data found - database is complete")
        
        else:
            parser.print_help()
    
    finally:
        checker.close()


if __name__ == "__main__":
    main()