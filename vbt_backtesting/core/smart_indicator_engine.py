"""
Smart Indicator Engine for Ultimate Efficiency System
====================================================

TAlib-based indicator engine with VectorBT IndicatorFactory integration.
Follows proven pattern from vectorbt_indicator_signal_check.py for maximum performance.

Key Features:
- Direct TAlib integration with NumPy fallbacks
- Smart calculation (only required indicators)
- VectorBT IndicatorFactory pattern for optimal performance
- 50-70% reduction in indicator calculations through dependency analysis
- Production-grade error handling and validation

Author: vAlgo Development Team
Created: July 29, 2025
"""

import sys
import warnings
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional, Union
from datetime import datetime

# Add parent directory to path for imports
sys.path.append(str(Path(__file__).parent.parent))

from core.json_config_loader import JSONConfigLoader, ConfigurationError
from core.dynamic_warmup_calculator import DynamicWarmupCalculator

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

# TAlib imports with fallback handling (following proven pattern)
try:
    import talib
    TALIB_AVAILABLE = True
    print(f"âœ… TAlib {talib.__version__} loaded successfully")
except ImportError:
    TALIB_AVAILABLE = False
    print("âŒ TAlib not available. Using fallback NumPy implementations")
    print("ðŸ’¡ Note: For production use, install TAlib with: pip install TA-Lib")

# VectorBT imports (silent loading - main import handles messaging)
try:
    import vectorbt as vbt
    VECTORBT_AVAILABLE = True
except ImportError:
    VECTORBT_AVAILABLE = False
    print("âŒ VectorBT not available. Install with: pip install vectorbt[full]")
    sys.exit(1)

# DuckDB imports for CPR database integration
try:
    import duckdb
    DUCKDB_AVAILABLE = True
except ImportError:
    DUCKDB_AVAILABLE = False
    print("âŒ DuckDB not available. CPR calculation will fail if database access is required.")

# vAlgo system imports - required for production

class SmartIndicatorEngine:
    """
    Smart indicator calculation engine with TAlib integration.
    
    Features:
    - Calculate only indicators required by active strategies (50-70% reduction)
    - Direct TAlib integration following proven patterns
    - VectorBT IndicatorFactory for optimal performance
    - Comprehensive fallback mechanisms
    - Production-grade validation and error handling
    """
    
    def __init__(self, config_loader: JSONConfigLoader):
        """
        Initialize Smart Indicator Engine.
        
        Args:
            config_loader: REQUIRED pre-initialized config loader
            
        Raises:
            ConfigurationError: If config_loader is None or configuration is invalid
        """
        if config_loader is None:
            raise ConfigurationError("SmartIndicatorEngine requires a valid JSONConfigLoader instance")
        
        self.config_loader = config_loader
        
        # Require proper logger from config - no fallbacks
        logger_config = self.config_loader.main_config.get('system', {}).get('logger_config', {})
        if not logger_config.get('enabled', False):
            raise ConfigurationError("Logger must be enabled in system.logger_config")
        
        
        # Load configurations
        self.main_config, self.strategies_config = self.config_loader.load_all_configs()
        
        # Check TAlib fallback configuration
        indicators_config = self.main_config.get('indicators', {})
        self.allow_numpy_fallback = indicators_config.get('allow_numpy_fallback', False)
        self.require_complete_calculation = indicators_config.get('require_complete_calculation', True)
        
        if not TALIB_AVAILABLE and not self.allow_numpy_fallback:
            raise ConfigurationError(
                "TAlib is not available and numpy fallback is disabled. "
                "Either install TAlib or set 'indicators.allow_numpy_fallback: true' in config.json"
            )
        
        # Extract simplified configuration (config-based approach)
        self.enabled_indicators = self.config_loader.get_enabled_indicators_from_config()
        self.all_indicators = self._get_all_available_indicators()
        
        # Performance tracking (simplified approach)
        self.calculation_stats = {
            'total_available': len(self.all_indicators),
            'enabled_count': len(self.enabled_indicators),
            'approach': 'config_based_simple',
            'calculation_method': 'enabled_indicators_only',
            'talib_available': TALIB_AVAILABLE,
            'numpy_fallback_allowed': self.allow_numpy_fallback
        }
        
        # Individual indicator timing tracking
        self.indicator_timings = {}
        
        # Create indicator calculation functions
        self.indicator_functions = self._create_indicator_functions()
        
        # Initialize warmup calculator for data validation
        self.warmup_calculator = DynamicWarmupCalculator(config_loader)
        
        # CPR calculation cache for ultra-fast performance
        self._cpr_cache = {}
        self._current_ohlcv_data = None
        
        # Simplified logging - avoid duplicate messages when used in multiple components
        if not hasattr(SmartIndicatorEngine, '_initialized_count'):
            SmartIndicatorEngine._initialized_count = 0
        SmartIndicatorEngine._initialized_count += 1
        
        # Only show detailed log for the first initialization
        if SmartIndicatorEngine._initialized_count == 1:
            # Get indicator summary from config loader for better display
            indicator_summary = self.config_loader.get_indicator_summary()
            
            print(f"ðŸ§  Smart Indicator Engine initialized")
            print(f"   ðŸ“Š Available indicators: {indicator_summary['available_count']} ({', '.join(indicator_summary['available_list'])})")
            print(f"   âœ… Enabled Indicators: {len(indicator_summary['enabled_list'])} ({', '.join(indicator_summary['enabled_list'])})")
            print(f"   ðŸ”‘ Created Indicator keys: {indicator_summary['total_keys']}")
            print(f"   ðŸ”— Total indicator keys: {indicator_summary['total_with_ohlcv']} (Enabled indicators + OHLCV)")
    
    def calculate_indicators(self, data: pd.DataFrame) -> Dict[str, pd.Series]:
        """
        Calculate only required indicators using smart dependency analysis.
        
        Args:
            data: OHLCV DataFrame with required columns
            
        Returns:
            Dictionary of calculated indicators {indicator_name: values}
            
        Raises:
            ConfigurationError: If data validation fails
        """
        start_time = datetime.now()
        
        # Validate input data
        self._validate_input_data(data)
        
        # Validate data sufficiency for warmup requirements
        self._validate_data_warmup_sufficiency(data)
        
        # Calculate all enabled indicators from config.json (simplified approach)
        calculated_indicators = {}
        
        # Define all market data keys (not calculable indicators)
        MARKET_DATA_KEYS = {
            'open', 'high', 'low', 'close', 'volume',  # Base 5m market data
            'ltp_open', 'ltp_high', 'ltp_low', 'ltp_close', 'ltp_volume',  # LTP market data (was 1m_*)
            'Indicator_Timestamp'  # Debugging column
        }
        
        # Process only enabled indicators from config (exclude market data keys)
        enabled_indicator_keys = [key for key in self.enabled_indicators.keys() 
                                 if key not in MARKET_DATA_KEYS]
        
        print(f"ðŸ”§ Calculating {len(enabled_indicator_keys)} enabled indicators...")
        
        for indicator_name in enabled_indicator_keys:
            try:
                # Track individual indicator timing
                indicator_start_time = datetime.now()
                indicator_values = self._calculate_single_indicator(indicator_name, data)
                indicator_end_time = datetime.now()
                
                # Store timing for this indicator
                indicator_duration = (indicator_end_time - indicator_start_time).total_seconds()
                self.indicator_timings[indicator_name] = indicator_duration
                
                calculated_indicators[indicator_name] = indicator_values
                
                # Validation logging with data type handling
                if indicator_values.dtype == object:
                    # String data (like cpr_type)
                    valid_count = np.sum(indicator_values != "")
                else:
                    # Numeric data (like cpr_pivot, cpr_range_type, etc.)
                    valid_count = np.sum(~np.isnan(indicator_values))
                print(f"   âœ… {indicator_name}: {valid_count} valid values ({indicator_duration:.3f}s)")
                
            except Exception as e:
                print(f"   âŒ Failed to calculate {indicator_name}: {e}")
                # Use fail-fast approach - don't continue with invalid indicators
                raise ConfigurationError(f"Indicator calculation failed for {indicator_name}: {e}")
        
        calculation_time = (datetime.now() - start_time).total_seconds()
        
        print(f"âš¡ Indicator calculation completed in {calculation_time:.3f}s")
        print(f"   ðŸ“Š Calculated {len(calculated_indicators)} enabled indicators using config-based approach")
        
        return calculated_indicators
    
    def get_vectorbt_indicators(self, data: pd.DataFrame) -> Dict[str, Any]:
        """
        Create VectorBT IndicatorFactory instances for required indicators.
        
        Args:
            data: OHLCV DataFrame
            
        Returns:
            Dictionary of VectorBT indicator instances
        """
        print("ðŸ­ Creating VectorBT IndicatorFactory instances...")
        
        vectorbt_indicators = {}
        
        for indicator_name in self.required_indicators:
            try:
                indicator_factory = self._create_vectorbt_indicator(indicator_name, data)
                vectorbt_indicators[indicator_name] = indicator_factory
                print(f"   ðŸ”§ Created VectorBT factory: {indicator_name}")
                
            except Exception as e:
                self.logger.error(f"Failed to create VectorBT indicator {indicator_name}: {e}")
                raise ConfigurationError(f"VectorBT indicator creation failed for {indicator_name}: {e}")
        
        print(f"âœ… Created {len(vectorbt_indicators)} VectorBT indicator factories")
        return vectorbt_indicators
    
    # =============================================================================
    # TAlib Integration Functions (Following Proven Pattern)
    # =============================================================================
    
    def _talib_rsi_apply_func(self, close: np.ndarray, period: int) -> np.ndarray:
        """
        TAlib-based RSI calculation following proven pattern.
        
        Args:
            close: Price series (numpy array or pandas Series)
            period: RSI period (typically 14 or 21)
            
        Returns:
            numpy array with RSI values (0-100 range)
        """
        # Convert to numpy array with proper dtype for TAlib
        close = np.asarray(close, dtype=np.float64).flatten()
        period = int(period)
        
        # Use TAlib RSI function if available, otherwise fallback to numpy implementation
        if TALIB_AVAILABLE:
            rsi_values = talib.RSI(close, timeperiod=period)
        else:
            # Fallback NumPy RSI calculation
            rsi_values = self._calculate_rsi_numpy(close, period)
        
        return rsi_values
    
    def _talib_sma_apply_func(self, close: np.ndarray, period: int) -> np.ndarray:
        """
        TAlib-based SMA calculation following proven pattern.
        
        Args:
            close: Price series (numpy array or pandas Series)
            period: SMA period (typically 9, 20, 50, 200)
            
        Returns:
            numpy array with SMA values
        """
        # Convert to numpy array with proper dtype for TAlib
        close = np.asarray(close, dtype=np.float64).flatten()
        period = int(period)
        
        # Use TAlib SMA function if available, otherwise fallback to numpy implementation
        if TALIB_AVAILABLE:
            sma_values = talib.SMA(close, timeperiod=period)
        else:
            # Fallback NumPy SMA calculation
            sma_values = self._calculate_sma_numpy(close, period)
        
        return sma_values
    
    def _talib_ema_apply_func(self, close: np.ndarray, period: int) -> np.ndarray:
        """
        TAlib-based EMA calculation following proven pattern.
        
        Args:
            close: Price series (numpy array or pandas Series)
            period: EMA period (typically 9, 20)
            
        Returns:
            numpy array with EMA values
        """
        # Convert to numpy array with proper dtype for TAlib
        close = np.asarray(close, dtype=np.float64).flatten()
        period = int(period)
        
        # Use TAlib EMA function if available, otherwise fallback to pandas
        if TALIB_AVAILABLE:
            ema_values = talib.EMA(close, timeperiod=period)
        else:
            # Fallback pandas EMA calculation
            close_series = pd.Series(close)
            ema_values = close_series.ewm(span=period).mean().values
        
        return ema_values
    
    # =============================================================================
    # Fallback NumPy Implementations (When TAlib Not Available)
    # =============================================================================
    
    def _calculate_rsi_numpy(self, close: np.ndarray, period: int) -> np.ndarray:
        """Fallback RSI calculation using NumPy (following proven pattern)."""
        try:
            delta = np.diff(close)
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            
            # Calculate exponential moving averages
            avg_gain = np.empty_like(close)
            avg_loss = np.empty_like(close)
            avg_gain[:] = np.nan
            avg_loss[:] = np.nan
            
            # Initial SMA
            if len(gain) >= period:
                avg_gain[period] = np.mean(gain[:period])
                avg_loss[period] = np.mean(loss[:period])
                
                # Subsequent EMA
                for i in range(period + 1, len(close)):
                    avg_gain[i] = (avg_gain[i-1] * (period - 1) + gain[i-1]) / period
                    avg_loss[i] = (avg_loss[i-1] * (period - 1) + loss[i-1]) / period
            
            # Calculate RSI
            rs = avg_gain / np.where(avg_loss == 0, 1e-10, avg_loss)
            rsi = 100 - (100 / (1 + rs))
            
            return rsi
        except Exception as e:
            # No default values - fail fast with proper error
            if self.require_complete_calculation:
                raise ConfigurationError(f"RSI calculation failed: {e}. Set indicators.require_complete_calculation to false to allow incomplete calculations.")
            else:
                self.logger.warning(f"RSI calculation failed, returning NaN values: {e}")
                return np.full_like(close, np.nan)
    
    def _calculate_sma_numpy(self, close: np.ndarray, period: int) -> np.ndarray:
        """Fallback SMA calculation using NumPy (following proven pattern)."""
        try:
            sma = np.empty_like(close)
            sma[:] = np.nan
            
            for i in range(period - 1, len(close)):
                sma[i] = np.mean(close[i - period + 1:i + 1])
            
            return sma
        except Exception as e:
            # No default values - fail fast with proper error
            if self.require_complete_calculation:
                raise ConfigurationError(f"SMA calculation failed: {e}. Set indicators.require_complete_calculation to false to allow incomplete calculations.")
            else:
                self.logger.warning(f"SMA calculation failed, returning NaN values: {e}")
                return np.full_like(close, np.nan)
    
    # =============================================================================
    # Ultra-Fast CPR Implementation with NumPy (Frankochavs Formula)
    # =============================================================================
    
    def _calculate_cpr_numpy_batch(self, close: np.ndarray) -> Dict[str, np.ndarray]:
        """
        Ultra-fast CPR calculation using pure NumPy batch processing.
        
        Uses Frankochavs CPR formulas with previous day (D-1) OHLC data.
        No warmup required since CPR only needs previous day's data.
        Fail-fast approach - no fallbacks, no hardcoded values.
        
        Args:
            close: Close price array (not used directly, but required for interface compatibility)
            
        Returns:
            Dictionary of all CPR indicators as NumPy arrays
            
        Raises:
            ConfigurationError: If calculation fails (fail-fast, no fallbacks)
        """
        print("ðŸš€ Ultra-Fast CPR Calculation: Starting NumPy batch processing...")
        
        # FAIL-FAST: Ensure OHLCV data is available
        if not hasattr(self, '_current_ohlcv_data') or self._current_ohlcv_data is None:
            raise ConfigurationError("OHLCV data not available for CPR calculation")
        
        data = self._current_ohlcv_data
        
        # Check cache first for ultra-fast performance
        data_hash = hash(tuple(data.index) + tuple(data['close'].values[:10]))
        if data_hash in self._cpr_cache:
            print("âš¡ Using cached CPR results (ultra-fast)")
            return self._cpr_cache[data_hash]
        
        # Extract dates for daily grouping
        if hasattr(data.index, 'date'):
            dates_array = np.array([d for d in data.index.date])
        else:
            dates_array = np.array([d.date() for d in pd.to_datetime(data.index)])
        
        print(f"   ðŸ“Š Processing {len(data)} candles for CPR calculation")
        
        # Try database-first approach for daily OHLC (most accurate)
        daily_ohlc = self._fetch_daily_candles_from_database()
        
        if daily_ohlc is None:
            # FAIL-FAST: No fallback to aggregation
            raise ConfigurationError(
                "Database unavailable for daily OHLC data. CPR calculation requires accurate daily candles. "
                "Ensure DuckDB database is available and contains daily OHLC data."
            )
        
        # Calculate CPR levels using vectorized NumPy operations
        cpr_results = self._calculate_cpr_levels_numpy_optimized(daily_ohlc)
        
        # Broadcast daily CPR values back to intraday timeframe
        cpr_intraday = self._broadcast_daily_cpr_to_intraday_optimized(cpr_results, dates_array, len(data))
        
        # Cache results for performance
        self._cpr_cache[data_hash] = cpr_intraday
        
        print(f"âœ… Ultra-Fast CPR Calculation: Completed with {len(cpr_results) - 1} CPR indicators")
        
        return cpr_intraday
    
    def _fetch_daily_candles_from_database(self) -> Optional[Dict[str, np.ndarray]]:
        """
        Fetch daily OHLC candles directly from DuckDB database for maximum accuracy.
        
        Returns:
            Dictionary with daily OHLC data as NumPy arrays, or None if database unavailable
            
        Raises:
            ConfigurationError: If database query fails (fail-fast approach)
        """
        if not DUCKDB_AVAILABLE:
            return None
        
        try:
            # Get database path from config
            db_path = self.main_config.get('database', {}).get('path', 'data/vAlgo_market_data.db')
            symbol = self.main_config.get('trading', {}).get('symbol', 'NIFTY')
            
            # FAIL-FAST: Database path must exist
            if not Path(db_path).exists():
                raise ConfigurationError(f"Database not found at: {db_path}")
            
            conn = duckdb.connect(db_path)
            
            # Ultra-fast daily aggregation query (matching existing CPR implementation)
            query = """
            SELECT 
                DATE(timestamp) as date,
                FIRST(open ORDER BY timestamp) as open,
                MAX(high) as high,
                MIN(low) as low,
                (SELECT close FROM ohlcv_data sub 
                 WHERE sub.symbol = ? 
                 AND DATE(sub.timestamp) = DATE(ohlcv_data.timestamp) 
                 ORDER BY 
                    CASE WHEN EXTRACT(hour FROM sub.timestamp) = 0 AND EXTRACT(minute FROM sub.timestamp) = 0 
                         THEN 0 ELSE 1 END,
                    sub.timestamp DESC 
                 LIMIT 1) as close,
                SUM(volume) as volume
            FROM ohlcv_data 
            WHERE symbol = ?
            GROUP BY DATE(timestamp)
            ORDER BY DATE(timestamp)
            """
            
            result = conn.execute(query, [symbol, symbol]).fetchall()
            conn.close()
            
            if not result:
                raise ConfigurationError(f"No daily OHLC data found for symbol: {symbol}")
            
            # Convert to NumPy arrays directly (no pandas overhead)
            dates = np.array([row[0] for row in result])
            opens = np.array([float(row[1]) for row in result])
            highs = np.array([float(row[2]) for row in result])
            lows = np.array([float(row[3]) for row in result])
            closes = np.array([float(row[4]) for row in result])
            # Note: row[5] would be volume, but we don't need it for CPR
            
            print(f"   ðŸ—„ï¸  Fetched {len(result)} daily candles from database")
            
            return {
                'dates': dates,
                'open': opens,
                'high': highs,
                'low': lows,
                'close': closes
            }
            
        except Exception as e:
            if "Database not found" in str(e) or "No daily OHLC data found" in str(e):
                raise  # Re-raise our ConfigurationError
            else:
                raise ConfigurationError(f"Database query failed for daily OHLC: {e}")
    
    def _calculate_cpr_levels_numpy_optimized(self, daily_ohlc: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """
        Calculate all CPR levels using ultra-fast NumPy vectorized operations.
        
        Implements Frankochavs CPR formulas with zero Python loops:
        - Pivot = (H + L + C) / 3
        - BC = (H + L) / 2  
        - TC = 2 * Pivot - BC
        - Support/Resistance levels using standard formulas
        
        Args:
            daily_ohlc: Dictionary with daily OHLC data as NumPy arrays
            
        Returns:
            Dictionary with all CPR levels as NumPy arrays
            
        Raises:
            ConfigurationError: If calculation fails (fail-fast approach)
        """
        # FAIL-FAST: Validate input data
        required_keys = ['dates', 'high', 'low', 'close']
        for key in required_keys:
            if key not in daily_ohlc:
                raise ConfigurationError(f"Missing required OHLC data: {key}")
        
        # Extract daily OHLC arrays (direct NumPy arrays)
        high = daily_ohlc['high']
        low = daily_ohlc['low']
        close = daily_ohlc['close']
        dates = daily_ohlc['dates']
        
        # FAIL-FAST: Ensure data consistency
        if len(high) != len(low) or len(low) != len(close) or len(close) != len(dates):
            raise ConfigurationError("OHLC array lengths are inconsistent")
        
        if len(high) < 2:
            raise ConfigurationError("Need at least 2 days of data for CPR calculation")
        
        # Previous day arrays using NumPy roll (ultra-fast shift operation)
        prev_high = np.roll(high, 1)
        prev_low = np.roll(low, 1)
        prev_close = np.roll(close, 1)
        
        # Set first day to NaN (no previous day data available)
        prev_high[0] = np.nan
        prev_low[0] = np.nan
        prev_close[0] = np.nan
        
        # FRANKOCHAVS CPR FORMULAS - Ultra-fast vectorized calculation for ALL dates at once
        # Core CPR levels (CORRECTED FORMULAS - all operations vectorized across entire array)
        pivot = (prev_high + prev_low + prev_close) / 3.0
        tc = (prev_high + prev_low) / 2.0  # Top Central (TC) = (H + L) / 2
        bc = 2.0 * pivot - tc  # Bottom Central (BC) = 2 * Pivot - TC
        
        # DEBUG: Show sample calculations for verification
        if len(dates) > 1 and not np.isnan(pivot[1]):
            sample_idx = 1  # Second day (first day with valid CPR)
            print(f"   ðŸ” CPR DEBUG - Sample calculation for {dates[sample_idx]}:")
            print(f"      Previous day OHLC: H={prev_high[sample_idx]:.2f}, L={prev_low[sample_idx]:.2f}, C={prev_close[sample_idx]:.2f}")
            print(f"      Calculated CPR: Pivot={pivot[sample_idx]:.2f}, TC={tc[sample_idx]:.2f}, BC={bc[sample_idx]:.2f}")
            print(f"      Formula check: 2*Pivot-TC = {2*pivot[sample_idx] - tc[sample_idx]:.2f} (should equal BC)")
            print(f"      CPR Width: {tc[sample_idx] - bc[sample_idx]:.2f}")
        
        # Support and Resistance levels - all calculated in single vectorized operations
        r1 = 2.0 * pivot - prev_low
        r2 = pivot + (prev_high - prev_low)
        r3 = prev_high + 2.0 * (pivot - prev_low)
        r4 = r3 + (r2 - r1)
        
        s1 = 2.0 * pivot - prev_high
        s2 = pivot - (prev_high - prev_low)
        s3 = prev_low - 2.0 * (prev_high - pivot)
        s4 = s3 - (s1 - s2)  # Frankochavs variation (not standard CPR)
        
        # CPR Width and Enhanced Range Type classification (vectorized)
        cpr_width = tc - bc
        
        # Enhanced CPR Range Type Classification using relative comparison
        # Based on current day vs previous day CPR ranges
        range_type = np.full(len(dates), 2, dtype=int)  # Default to "normal" (2)
        
        # Only classify from second day onwards (need previous day data)
        if len(dates) > 1:
            # Calculate ranges for vectorized comparison
            # Previous day ranges (exclude last day)
            pd_full = tc[:-1] - bc[:-1]  # Previous day full range (top - bottom)
            pd_half = tc[:-1] - pivot[:-1]  # Previous day half range (top - center)
            
            # Current day ranges (exclude first day) 
            cd_full = tc[1:] - bc[1:]  # Current day full range
            cd_half = tc[1:] - pivot[1:]  # Current day half range
            
            # Apply enhanced classification logic (vectorized)
            # 0=extreme_narrow, 1=narrow, 2=normal, 3=wide, 4=extreme_wide
            
            # Extreme Narrow: cd_full < pd_half
            extreme_narrow_mask = cd_full < pd_half
            range_type[1:][extreme_narrow_mask] = 0
            
            # Narrow: pd_full >= cd_full <= pd_half (and not extreme_narrow)
            narrow_mask = (pd_full >= cd_full) & (cd_full <= pd_full) & (cd_full >= pd_half) & (~extreme_narrow_mask)
            range_type[1:][narrow_mask] = 1
            
            # Extreme Wide: cd_half >= pd_full
            extreme_wide_mask = cd_half >= pd_full
            range_type[1:][extreme_wide_mask] = 4
            
            # Wide: cd_full >= pd_full >= cd_half (and not extreme_wide)
            wide_mask = (cd_full >= pd_full) & (pd_full <= cd_full) & (cd_half <= pd_full) & (~extreme_wide_mask)
            range_type[1:][wide_mask] = 3
            
        # Normal: everything else (already set as default)
        
        # Count classifications for analysis
        valid_mask = ~np.isnan(pivot)
        valid_count = np.sum(valid_mask)
        
        # Count each range type
        extreme_narrow_count = np.sum((range_type == 0) & valid_mask)
        narrow_count = np.sum((range_type == 1) & valid_mask)
        normal_count = np.sum((range_type == 2) & valid_mask)
        wide_count = np.sum((range_type == 3) & valid_mask)
        extreme_wide_count = np.sum((range_type == 4) & valid_mask)
        
        print(f"   ðŸ§® CPR Levels calculated for {valid_count}/{len(dates)} days using Frankochavs formulas")
        print(f"   ðŸ“ Enhanced Range Analysis:")
        print(f"      ðŸ”¹ Extreme Narrow: {extreme_narrow_count} days")
        print(f"      ðŸ”¸ Narrow: {narrow_count} days") 
        print(f"      âšª Normal: {normal_count} days")
        print(f"      ðŸ”¶ Wide: {wide_count} days")
        print(f"      ðŸ”´ Extreme Wide: {extreme_wide_count} days")
        
        # Create string representation of range types for readability
        range_type_names = np.array(['extreme_narrow', 'narrow', 'normal', 'wide', 'extreme_wide'])
        range_type_strings = range_type_names[range_type]
        
        return {
            'dates': dates,
            'cpr_pivot': pivot,
            'cpr_tc': tc,
            'cpr_bc': bc,
            'cpr_r1': r1,
            'cpr_r2': r2,
            'cpr_r3': r3,
            'cpr_r4': r4,
            'cpr_s1': s1,
            'cpr_s2': s2,
            'cpr_s3': s3,
            'cpr_s4': s4,
            'cpr_width': cpr_width,
            'cpr_range_type': range_type,
            'cpr_type': range_type_strings,  # Simple string type for easy use
            'previous_day_high': prev_high,
            'previous_day_low': prev_low,
            'previous_day_close': prev_close
        }
    
    def _broadcast_daily_cpr_to_intraday_optimized(self, cpr_results: Dict[str, np.ndarray], 
                                                  intraday_dates: np.ndarray, 
                                                  intraday_length: int) -> Dict[str, np.ndarray]:
        """
        Broadcast daily CPR values to intraday timeframe using ultra-fast NumPy vectorized operations.
        
        Args:
            cpr_results: Dictionary with daily CPR data
            intraday_dates: Array of dates for each intraday candle
            intraday_length: Length of intraday data
            
        Returns:
            Dictionary with CPR indicators broadcast to intraday timeframe
            
        Raises:
            ConfigurationError: If broadcasting fails (fail-fast approach)
        """
        daily_dates = cpr_results['dates']
        
        # FAIL-FAST: Validate inputs
        if len(intraday_dates) != intraday_length:
            raise ConfigurationError("Intraday dates array length mismatch")
        
        # CPR indicators to broadcast (all 17 indicators)
        cpr_indicators = [
            'cpr_pivot', 'cpr_tc', 'cpr_bc', 'cpr_r1', 'cpr_r2', 'cpr_r3', 'cpr_r4',
            'cpr_s1', 'cpr_s2', 'cpr_s3', 'cpr_s4', 'cpr_width', 'cpr_range_type', 'cpr_type',
            'previous_day_high', 'previous_day_low', 'previous_day_close'
        ]
        
        # Initialize output arrays with appropriate data types
        intraday_cpr = {}
        for indicator in cpr_indicators:
            if indicator not in cpr_results:
                raise ConfigurationError(f"Missing CPR indicator: {indicator}")
            
            # Check data type and initialize appropriately
            sample_value = cpr_results[indicator][0] if len(cpr_results[indicator]) > 0 else None
            if isinstance(sample_value, (str, np.str_)):
                # String data (like cpr_type)
                intraday_cpr[indicator] = np.full(intraday_length, "", dtype=object)
            else:
                # Numeric data (like cpr_pivot, cpr_range_type, etc.)
                intraday_cpr[indicator] = np.full(intraday_length, np.nan)
        
        # ULTRA-FAST VECTORIZED BROADCASTING
        # Create date mapping using NumPy for maximum performance
        try:
            # Convert dates to comparable format if needed
            if len(daily_dates) > 0:
                # Create boolean mask for each daily date (vectorized approach)
                for daily_idx, daily_date in enumerate(daily_dates):
                    # Find all intraday candles for this date (vectorized comparison)
                    intraday_mask = intraday_dates == daily_date
                    
                    if np.any(intraday_mask):
                        # Broadcast all CPR values for this date at once (vectorized assignment)
                        for indicator in cpr_indicators:
                            intraday_cpr[indicator][intraday_mask] = cpr_results[indicator][daily_idx]
            
            # Count successful broadcasts (use numeric indicator for counting)
            non_nan_count = np.sum(~np.isnan(intraday_cpr['cpr_pivot']))
            broadcast_rate = (non_nan_count / intraday_length) * 100
            
            print(f"   ðŸ“¡ CPR values broadcast to {non_nan_count}/{intraday_length} intraday candles ({broadcast_rate:.1f}%)")
            
            if broadcast_rate < 50:
                print(f"   âš ï¸  Warning: Low CPR broadcast rate ({broadcast_rate:.1f}%) - check date alignment")
            
            return intraday_cpr
            
        except Exception as e:
            raise ConfigurationError(f"CPR broadcasting failed: {e}")
    
    # =============================================================================
    # Previous Candle OHLC Implementation
    # =============================================================================
    
    def _calculate_previous_candle_ohlc(self, close: np.ndarray) -> Dict[str, np.ndarray]:
        """
        Calculate previous candle OHLC data using NumPy shift operations.
        
        For 5m timeframe:
        - At 9:20, previous candle returns 9:15 OHLC data
        - At 9:15, previous candle returns NaN (no previous data)
        
        Args:
            close: Close price array (not used directly, but required for interface compatibility)
            
        Returns:
            Dictionary with previous candle OHLC data as NumPy arrays
            
        Raises:
            ConfigurationError: If calculation fails
        """
        print("ðŸ• Previous Candle OHLC Calculation: Starting NumPy shift processing...")
        
        # FAIL-FAST: Ensure OHLCV data is available
        if not hasattr(self, '_current_ohlcv_data') or self._current_ohlcv_data is None:
            raise ConfigurationError("OHLCV data not available for previous candle calculation")
        
        data = self._current_ohlcv_data
        
        # Extract OHLC data
        open_data = data['open'].values
        high_data = data['high'].values
        low_data = data['low'].values
        close_data = data['close'].values
        
        # Shift each OHLC series by 1 period to get previous candle data
        # np.roll with shift=1 moves data forward by 1 position
        previous_open = np.roll(open_data, 1)
        previous_high = np.roll(high_data, 1)
        previous_low = np.roll(low_data, 1)
        previous_close = np.roll(close_data, 1)
        
        # Set first candle to NaN (no previous data available)
        previous_open[0] = np.nan
        previous_high[0] = np.nan
        previous_low[0] = np.nan
        previous_close[0] = np.nan
        
        # Count valid (non-NaN) values
        valid_count = len(data) - 1  # All except first candle
        
        print(f"   ðŸ“Š Previous candle OHLC calculated for {valid_count}/{len(data)} candles")
        print(f"   âš ï¸  First candle (index 0) set to NaN - no previous data available")
        
        return {
            'previous_candle_open': previous_open,
            'previous_candle_high': previous_high,
            'previous_candle_low': previous_low,
            'previous_candle_close': previous_close
        }
    
    # =============================================================================
    # Internal Helper Methods
    # =============================================================================
    
    def _validate_input_data(self, data: pd.DataFrame):
        """Validate input OHLCV data."""
        required_columns = ['open', 'high', 'low', 'close', 'volume']
        missing_columns = [col for col in required_columns if col not in data.columns]
        
        if missing_columns:
            raise ConfigurationError(f"Missing required columns: {missing_columns}")
        
        if len(data) == 0:
            raise ConfigurationError("Input data is empty")
        
        # Check for data quality issues
        if data['close'].isna().all():
            raise ConfigurationError("All close prices are NaN")
    
    def _get_all_available_indicators(self) -> List[str]:
        """Get list of all available indicators from configuration."""
        all_indicators = []
        
        for indicator_name, indicator_config in self.main_config['indicators'].items():
            # Handle both boolean and dictionary configurations
            if isinstance(indicator_config, bool):
                if indicator_config:  # If True, add the indicator
                    all_indicators.append(indicator_name)
            elif isinstance(indicator_config, dict):
                if indicator_config.get('enabled', False):
                    if 'periods' in indicator_config:
                        for period in indicator_config['periods']:
                            all_indicators.append(f"{indicator_name}_{period}")
                    else:
                        all_indicators.append(indicator_name)
            else:
                # Skip invalid configuration types
                continue
        
        return all_indicators
    
    def _create_indicator_functions(self) -> Dict[str, Any]:
        """Create indicator calculation functions mapping."""
        functions = {}
        
        # Map indicator types to calculation functions
        indicator_mappings = {
            'rsi': self._talib_rsi_apply_func,
            'sma': self._talib_sma_apply_func,
            'ema': self._talib_ema_apply_func,
            'cpr': self._calculate_cpr_numpy_batch,
            'previous_candle_ohlc': self._calculate_previous_candle_ohlc
        }
        
        return indicator_mappings
    
    def _calculate_single_indicator(self, indicator_name: str, data: pd.DataFrame) -> pd.Series:
        """Calculate a single indicator."""
        
        # SPECIAL HANDLING FOR CPR SUB-INDICATORS (check first before parsing)
        if indicator_name.startswith('cpr_'):
            # Store current data for CPR calculation
            self._current_ohlcv_data = data
            
            # Check if we've already calculated CPR for this data
            data_hash = hash(tuple(data.index) + tuple(data['close'].values[:10]))
            
            if data_hash not in self._cpr_cache:
                # Calculate all CPR indicators at once
                cpr_calc_function = self.indicator_functions.get('cpr')
                if cpr_calc_function is None:
                    raise ConfigurationError("CPR calculation function not found")
                
                print(f"ðŸš€ Ultra-Fast CPR Calculation: Starting for {indicator_name}...")
                self._cpr_cache[data_hash] = cpr_calc_function(data['close'].values)
            
            # Return the specific CPR sub-indicator requested
            cpr_results = self._cpr_cache[data_hash]
            if indicator_name in cpr_results:
                return pd.Series(cpr_results[indicator_name], index=data.index, name=indicator_name)
            else:
                available_keys = list(cpr_results.keys())
                raise ConfigurationError(f"CPR sub-indicator '{indicator_name}' not found. Available: {available_keys}")
        
        # SPECIAL HANDLING FOR PREVIOUS DAY INDICATORS (renamed CPR sub-indicators)
        if indicator_name.startswith('previous_day_'):
            # Store current data for CPR calculation
            self._current_ohlcv_data = data
            
            # Check if we've already calculated CPR for this data
            data_hash = hash(tuple(data.index) + tuple(data['close'].values[:10]))
            
            if data_hash not in self._cpr_cache:
                # Calculate all CPR indicators at once
                cpr_calc_function = self.indicator_functions.get('cpr')
                if cpr_calc_function is None:
                    raise ConfigurationError("CPR calculation function not found")
                
                print(f"ðŸš€ Ultra-Fast CPR Calculation: Starting for {indicator_name}...")
                self._cpr_cache[data_hash] = cpr_calc_function(data['close'].values)
            
            # Return the specific previous day sub-indicator requested
            cpr_results = self._cpr_cache[data_hash]
            if indicator_name in cpr_results:
                return pd.Series(cpr_results[indicator_name], index=data.index, name=indicator_name)
            else:
                available_keys = list(cpr_results.keys())
                raise ConfigurationError(f"Previous day sub-indicator '{indicator_name}' not found. Available: {available_keys}")
        
        # SPECIAL HANDLING FOR PREVIOUS CANDLE OHLC SUB-INDICATORS  
        if indicator_name.startswith('previous_candle_'):
            # Store current data for previous candle calculation
            self._current_ohlcv_data = data
            
            # Check if we've already calculated previous candle OHLC for this data
            data_hash = hash(tuple(data.index) + tuple(data['close'].values[:10]))
            cache_key = f"prev_candle_{data_hash}"
            
            if cache_key not in self._cpr_cache:
                # Calculate all previous candle OHLC indicators at once
                prev_calc_function = self.indicator_functions.get('previous_candle_ohlc')
                if prev_calc_function is None:
                    raise ConfigurationError("Previous candle OHLC calculation function not found")
                
                print(f"ðŸ• Previous Candle Calculation: Starting for {indicator_name}...")
                self._cpr_cache[cache_key] = prev_calc_function(data['close'].values)
            
            # Return the specific previous candle sub-indicator requested
            prev_results = self._cpr_cache[cache_key]
            if indicator_name in prev_results:
                return pd.Series(prev_results[indicator_name], index=data.index, name=indicator_name)
            else:
                available_keys = list(prev_results.keys())
                raise ConfigurationError(f"Previous candle sub-indicator '{indicator_name}' not found. Available: {available_keys}")
        
        # Parse indicator name for standard indicators (e.g., "rsi_14" -> type="rsi", period=14)
        if '_' in indicator_name:
            indicator_type, period_str = indicator_name.split('_', 1)
            try:
                period = int(period_str)
            except ValueError:
                # For non-CPR indicators that can't parse period
                indicator_type = indicator_name
                period = None
        else:
            indicator_type = indicator_name
            period = None
        
        # Get calculation function for standard indicators
        calc_function = self.indicator_functions.get(indicator_type)
        if calc_function is None:
            raise ConfigurationError(f"Unsupported indicator type: {indicator_type}")
        
        # Standard indicator calculation
        close_prices = data['close'].values
        
        if period is not None:
            indicator_values = calc_function(close_prices, period)
        else:
            # For indicators without periods (like VWAP)
            indicator_values = calc_function(close_prices)
        
        # Convert to pandas Series with original index
        return pd.Series(indicator_values, index=data.index, name=indicator_name)
    
    def _create_vectorbt_indicator(self, indicator_name: str, data: pd.DataFrame) -> Any:
        """Create VectorBT IndicatorFactory for a specific indicator."""
        # Parse indicator details
        if '_' in indicator_name:
            indicator_type, period_str = indicator_name.split('_', 1)
            period = int(period_str)
        else:
            indicator_type = indicator_name
            period = None
        
        # Create VectorBT IndicatorFactory following proven pattern
        if indicator_type == 'rsi':
            def rsi_apply_func(close):
                return self._talib_rsi_apply_func(close, period)
            
            indicator_factory = vbt.IndicatorFactory(
                input_names=['close'],
                param_names=['period'],
                output_names=[f'rsi_{period}']
            ).from_apply_func(
                rsi_apply_func,
                period=period,
                keep_pd=True
            )
            
        elif indicator_type == 'sma':
            def sma_apply_func(close):
                return self._talib_sma_apply_func(close, period)
            
            indicator_factory = vbt.IndicatorFactory(
                input_names=['close'],
                param_names=['period'],
                output_names=[f'{indicator_type}_{period}']
            ).from_apply_func(
                sma_apply_func,
                period=period,
                keep_pd=True
            )
            
        elif indicator_type == 'ema':
            def ema_apply_func(close):
                return self._talib_ema_apply_func(close, period)
            
            indicator_factory = vbt.IndicatorFactory(
                input_names=['close'],
                param_names=['period'],
                output_names=[f'ema_{period}']
            ).from_apply_func(
                ema_apply_func,
                period=period,
                keep_pd=True
            )
            
        else:
            raise ConfigurationError(f"VectorBT factory not implemented for: {indicator_type}")
        
        # Run the indicator on data
        indicator_result = indicator_factory.run(data['close'])
        
        return indicator_result
    
    def get_calculation_summary(self) -> Dict[str, Any]:
        """Get summary of indicator calculation efficiency."""
        return {
            'total_available_indicators': self.calculation_stats['total_available'],
            'enabled_indicators': self.calculation_stats['enabled_count'],
            'approach': self.calculation_stats['approach'],
            'calculation_method': self.calculation_stats['calculation_method'],
            'talib_available': TALIB_AVAILABLE,
            'vectorbt_available': VECTORBT_AVAILABLE,
            'efficiency_enabled': self.main_config['performance']['enable_smart_indicators']
        }
    
    def get_indicator_timings(self) -> Dict[str, float]:
        """
        Get individual indicator calculation times.
        
        Returns:
            Dictionary mapping indicator names to calculation times in seconds
        """
        return self.indicator_timings.copy()
    
    def get_timing_summary(self) -> Dict[str, Any]:
        """
        Get comprehensive timing summary for all indicators.
        
        Returns:
            Dictionary with timing statistics and breakdown
        """
        if not self.indicator_timings:
            return {
                'total_indicators': 0,
                'total_time': 0.0,
                'average_time': 0.0,
                'fastest_indicator': None,
                'slowest_indicator': None,
                'indicator_timings': {}
            }
        
        timings = list(self.indicator_timings.values())
        total_time = sum(timings)
        average_time = total_time / len(timings)
        
        # Find fastest and slowest indicators
        fastest_indicator = min(self.indicator_timings.items(), key=lambda x: x[1])
        slowest_indicator = max(self.indicator_timings.items(), key=lambda x: x[1])
        
        return {
            'total_indicators': len(self.indicator_timings),
            'total_time': total_time,
            'average_time': average_time,
            'fastest_indicator': {'name': fastest_indicator[0], 'time': fastest_indicator[1]},
            'slowest_indicator': {'name': slowest_indicator[0], 'time': slowest_indicator[1]},
            'indicator_timings': self.indicator_timings.copy()
        }
    
    def get_warmup_requirements(self, start_date: Union[str, datetime]) -> Dict[str, Any]:
        """
        Get warmup requirements for current indicator configuration.
        
        Args:
            start_date: Desired backtest start date
            
        Returns:
            Dictionary with warmup analysis and extended start date
        """
        return self.warmup_calculator.calculate_extended_start_date(start_date)
    
    def validate_data_for_backtesting(self, data_start: Union[str, datetime], 
                                    backtest_start: Union[str, datetime]) -> Dict[str, Any]:
        """
        Validate if available data is sufficient for accurate backtesting.
        
        Args:
            data_start: Earliest available data date
            backtest_start: Desired backtest start date
            
        Returns:
            Dictionary with validation results
            
        Raises:
            ConfigurationError: If validation enabled and data insufficient
        """
        return self.warmup_calculator.validate_data_sufficiency(data_start, backtest_start)
    
    def _validate_data_warmup_sufficiency(self, data: pd.DataFrame):
        """Validate data length meets warmup requirements."""
        if len(data) == 0:
            raise ConfigurationError("Cannot validate warmup with empty data")
        
        # Get warmup requirements
        warmup_analysis = self.warmup_calculator.calculate_max_warmup_needed()
        required_candles = warmup_analysis['max_warmup_candles']
        
        # Check if we have enough data
        available_candles = len(data)
        
        if available_candles < required_candles:
            shortage = required_candles - available_candles
            raise ConfigurationError(
                f"Insufficient data for indicator warmup. "
                f"Required: {required_candles} candles, Available: {available_candles} candles. "
                f"Shortage: {shortage} candles ({shortage / self.warmup_calculator.candles_per_day:.1f} trading days). "
                f"Extend data loading period or reduce accuracy level."
            )
        
        # Calculate confidence level
        confidence = min(100.0, (available_candles / required_candles) * 100)
        print(f"âœ… Warmup validation passed: {confidence:.1f}% confidence ({available_candles}/{required_candles} candles)")


# Convenience function for external use
def create_smart_indicator_engine(config_dir: str) -> SmartIndicatorEngine:
    """
    Create and initialize Smart Indicator Engine.
    
    Args:
        config_dir: REQUIRED configuration directory path
        
    Returns:
        Initialized SmartIndicatorEngine instance
        
    Raises:
        ConfigurationError: If config_dir is not provided
    """
    if not config_dir:
        raise ConfigurationError("Configuration directory is required for create_smart_indicator_engine()")
    
    config_loader = JSONConfigLoader(config_dir)
    return SmartIndicatorEngine(config_loader)


if __name__ == "__main__":
    # Test Smart Indicator Engine
    try:
        print("ðŸ§ª Testing Smart Indicator Engine...")
        
        # Create engine
        config_dir = "config"
        engine = create_smart_indicator_engine(config_dir)
        
        # Display summary
        summary = engine.get_calculation_summary()
        print("\nðŸ“‹ Smart Indicator Engine Summary:")
        for key, value in summary.items():
            print(f"   {key}: {value}")
        
        # Test warmup requirements
        print("\nðŸ”¥ Testing warmup requirements...")
        warmup_reqs = engine.get_warmup_requirements("2025-06-01")
        print(f"âœ… Warmup requirements calculated successfully")
        print(f"   Extended start: {warmup_reqs['extended_start_date'].strftime('%Y-%m-%d')}")
        print(f"   Extension days: {warmup_reqs['days_extended']}")
        
        # Test with sample data
        print("\nðŸ”§ Testing with sample data...")
        # Create longer sample data to meet warmup requirements
        data_length = warmup_reqs['warmup_analysis']['max_warmup_candles'] + 100
        sample_data = pd.DataFrame({
            'open': np.random.randn(data_length).cumsum() + 100,
            'high': np.random.randn(data_length).cumsum() + 102,
            'low': np.random.randn(data_length).cumsum() + 98,
            'close': np.random.randn(data_length).cumsum() + 100,
            'volume': np.random.randint(1000, 10000, data_length)
        })
        
        # Calculate indicators
        indicators = engine.calculate_indicators(sample_data)
        print(f"âœ… Calculated {len(indicators)} indicators successfully")
        
        print("ðŸŽ‰ Smart Indicator Engine test passed!")
        
    except Exception as e:
        print(f"âŒ Smart Indicator Engine test failed: {e}")
        sys.exit(1)